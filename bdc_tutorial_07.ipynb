{
    "metadata": {
        "kernelspec": {
            "name": "pysparkkernel",
            "display_name": "PySpark"
        },
        "language_info": {
            "name": "pyspark",
            "mimetype": "text/x-python",
            "codemirror_mode": {
                "name": "python",
                "version": 2
            },
            "pygments_lexer": "python2"
        }
    },
    "nbformat_minor": 2,
    "nbformat": 4,
    "cells": [
        {
            "cell_type": "markdown",
            "source": [
                "<img src=\"https://github.com/Microsoft/sqlworkshops/blob/master/graphics/solutions-microsoft-logo-small.png?raw=true\" alt=\"Microsoft\">\r\n",
                "<br>\r\n",
                "\r\n",
                "# **SQL Server 2019 big data cluster Tutorial**\r\n",
                "## **07 - Using Spark For Machine Learning**\r\n",
                "\r\n",
                "In this tutorial you will learn how to train Spark ML model in a SQL Server big data cluster and export is as a MLeap bundle. \r\n",
                "\r\n",
                "Wide World Importers has refridgerated trucks to deliver temperature-sensitive products. These are high-profit, and high-expense items. In the past, there have been failures in the cooling systems, and the primary culprit has been the deep-cycle batteries used in the system.\r\n",
                "\r\n",
                "WWI began replacing the batteriess every three months as a preventative measure, but this has a high cost. Recently, the taxes on recycling batteries has increased dramatically. The CEO has asked the Data Science team if they can investigate creating a Predictive Maintenance system to more accurately tell the maintenance staff how long a battery will last, rather than relying on a flat 3 month cycle. \r\n",
                "\r\n",
                "The trucks have sensors that transmit data to a file location. The trips are also logged. In this Jupyter Notebook, you'll create, train and store a Machine Learning model using SciKit-Learn, so that it can be deployed to multiple hosts. \r\n",
                "\r\n",
                "> Switch your kernel to PySpark and run print(\"hello\") or whatever you like to activate Spark context.  \r\n",
                ">\r\n",
                "> If it output error like this\r\n",
                "> ```\r\n",
                "> The code failed because of a fatal error:\r\n",
                ">   Error sending http request and maximum retry encountered..\r\n",
                "> ```\r\n",
                "> Please switch to another kernel and switch back, and run again.\r\n",
                ""
            ],
            "metadata": {
                "azdata_cell_guid": "969bbd54-5f8e-49eb-b466-5e05633fa7be"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "print(\"hello\")"
            ],
            "metadata": {
                "azdata_cell_guid": "f7398c14-8f4b-4cba-bad9-be7f45ed6765"
            },
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "code",
            "source": [
                "## train a pyspark model and export it as a mleap bundle\r\n",
                "import os\r\n",
                "import numpy as np\r\n",
                "\r\n",
                "# parse command line arguments\r\n",
                "import argparse\r\n",
                "parser = argparse.ArgumentParser(description = 'train pyspark model and export mleap bundle')\r\n",
                "parser.add_argument('hdfs_path', nargs='?', default = \"/spark_ml\", type = str)\r\n",
                "parser.add_argument('model_name_export', nargs='?', default = \"battery_life_pipeline.zip\", type = str)\r\n",
                "args = parser.parse_args()\r\n",
                "\r\n",
                "hdfs_path = args.hdfs_path\r\n",
                "model_name_export = args.model_name_export"
            ],
            "metadata": {
                "azdata_cell_guid": "b6f5169c-8873-4059-90c6-20a4ea473604"
            },
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "code",
            "source": [
                "# read the data into a spark data frame.\r\n",
                "cwd = os.getcwd()\r\n",
                "filename = \"sensor-data.csv\"\r\n",
                "\r\n",
                "## NOTE: reading text file from local file path seems flaky!\r\n",
                "#import urllib.request\r\n",
                "#url = \"https://amldockerdatasets.azureedge.net/\" + filename\r\n",
                "#local_filename, headers = urllib.request.urlretrieve(url, filename)\r\n",
                "#datafile = \"file://\" + os.path.join(cwd, filename)\r\n",
                "\r\n",
                "data_all = spark.read\\\r\n",
                "    .options(\r\n",
                "        header='true', \r\n",
                "        inferSchema='true', \r\n",
                "        ignoreLeadingWhiteSpace='true', \r\n",
                "        ignoreTrailingWhiteSpace='true')\\\r\n",
                "    .csv(filename) #.load(datafile) for local file\r\n",
                "\r\n",
                "print(\"Number of rows: {},  Number of coulumns : {}\".format(data_all.count(), len(data_all.columns)))"
            ],
            "metadata": {
                "azdata_cell_guid": "fcd76dbf-0418-402e-8c07-9cc6266dc317"
            },
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "code",
            "source": [
                "data_all.printSchema() "
            ],
            "metadata": {
                "azdata_cell_guid": "fb469f1c-a4da-48ae-87f0-f9bf8a2e42bc"
            },
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "markdown",
            "source": [
                "First, download the sensor data from the location where it is transmitted from the trucks, and load it into a Spark DataFrame."
            ],
            "metadata": {
                "azdata_cell_guid": "18f250c3-d1fd-40e1-a652-10f948c8b0ab"
            }
        },
        {
            "cell_type": "markdown",
            "source": [
                "After examining the data, the Data Science team selects certain columns that they believe are highly predictive of the battery life."
            ],
            "metadata": {
                "azdata_cell_guid": "12249759-7afa-402a-badc-9167ad70b5f1"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "# choose feature columns and the label column for training.\r\n",
                "label = data_all.columns[0]\r\n",
                "xvars = data_all.columns[2:7]\r\n",
                "xvars.extend(data_all.columns[9:73])\r\n",
                "\r\n",
                "print(\"label: {}, features: {}\".format(label, xvars))\r\n",
                "\r\n",
                "select_cols = xvars\r\n",
                "select_cols.append(label)\r\n",
                "data = data_all.select(select_cols)"
            ],
            "metadata": {
                "azdata_cell_guid": "4a3781e2-1fb7-46f2-84c1-e23b74b769ae"
            },
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "code",
            "source": [
                "## split data into train and test.\r\n",
                "\r\n",
                "train, test = data.randomSplit([0.75, 0.25], seed=123)\r\n",
                "\r\n",
                "print(\"train ({}, {})\".format(train.count(), len(train.columns)))\r\n",
                "print(\"test ({}, {})\".format(test.count(), len(test.columns)))\r\n",
                "\r\n",
                "train_data_path = os.path.join(hdfs_path, \"sensor-data-train\")\r\n",
                "test_data_path = os.path.join(hdfs_path, \"sensor-data-test\")\r\n",
                "\r\n",
                "# write the train and test data sets to intermediate storage and then read\r\n",
                "train.write.mode('overwrite').orc(train_data_path)\r\n",
                "test.write.mode('overwrite').orc(test_data_path)\r\n",
                "\r\n",
                "print(\"train and test datasets saved to {} and {}\".format(train_data_path, test_data_path))\r\n",
                "\r\n",
                "train_read = spark.read.orc(train_data_path)\r\n",
                "test_read = spark.read.orc(test_data_path)\r\n",
                "\r\n",
                "assert train_read.schema == train.schema and train_read.count() == train.count() \r\n",
                "assert test_read.schema == test.schema and test_read.count() == test.count()\r\n",
                ""
            ],
            "metadata": {
                "azdata_cell_guid": "a0599280-eb9f-4c84-9080-c3bf4197b6fc"
            },
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "markdown",
            "source": [
                "The lead Data Scientist believes that a standard Regression algorithm would do the best predictions."
            ],
            "metadata": {
                "azdata_cell_guid": "db3206c7-c387-4fae-83fa-034fee6db6fa"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "from pyspark.ml import Pipeline, PipelineModel\r\n",
                "from pyspark.ml.feature import StringIndexer, VectorAssembler\r\n",
                "from pyspark.ml.regression import GBTRegressor\r\n",
                "from pyspark.ml.evaluation import RegressionEvaluator\r\n",
                "\r\n",
                "# Convert columns with string type to indexed string type\r\n",
                "str_indexer_1 = StringIndexer(inputCol=\"Region\", outputCol=\"Reg\")\r\n",
                "str_indexer_2 = StringIndexer(inputCol=\"Manufacture_Year\", outputCol=\"Manu_Year\")\r\n",
                "\r\n",
                "# Add newly created columns\r\n",
                "xvars.append(\"Reg\")\r\n",
                "xvars.append(\"Manu_Year\")\r\n",
                "\r\n",
                "# Remove duplicate columns\r\n",
                "xvars.remove(\"Region\")\r\n",
                "xvars.remove(\"Manufacture_Year\")\r\n",
                "\r\n",
                "# Assemble the encoded feature columns in to a column named \"features\"\r\n",
                "assembler = VectorAssembler(inputCols=xvars, outputCol=\"features\", handleInvalid=\"skip\")\r\n",
                "\r\n",
                "# Create a Gradient Boosted Tree Reression model, which by default uses \"features\" and \"label\" columns for training\r\n",
                "gbt = GBTRegressor(labelCol=label)\r\n",
                "\r\n",
                "# Put together the pipeline\r\n",
                "stages = []\r\n",
                "stages.append(str_indexer_1)\r\n",
                "stages.append(str_indexer_2)\r\n",
                "stages.append(assembler)\r\n",
                "stages.append(gbt)\r\n",
                "\r\n",
                "pipe = Pipeline(stages=stages)\r\n",
                "print(\"Pipeline created\")\r\n",
                "\r\n",
                "# Train the model.\r\n",
                "model = pipe.fit(train_read)\r\n",
                "print(\"Model Trained\")\r\n",
                "print(\"Model is \", model)\r\n",
                "print(\"Model Stages\", model.stages)"
            ],
            "metadata": {
                "azdata_cell_guid": "d6da7ab5-ea75-4e16-9655-a3c86f63da9a"
            },
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "code",
            "source": [
                "# Try making a single prediction and observe the result \r\n",
                "pred = model.transform(test_read)"
            ],
            "metadata": {
                "azdata_cell_guid": "11818008-b450-4799-a779-a1b6de81093b"
            },
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "markdown",
            "source": [
                "After the model is trained, perform testing from labeled data."
            ],
            "metadata": {
                "azdata_cell_guid": "0e6b111e-8e2f-41bc-a4f2-ed0620b147d6"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "from pyspark.ml.evaluation import RegressionEvaluator\r\n",
                "evaluator = RegressionEvaluator(predictionCol=\"prediction\", labelCol=label)\r\n",
                "print(\"Root mean squared error(RMSE) on test data = %g\" % evaluator.evaluate(pred))"
            ],
            "metadata": {
                "azdata_cell_guid": "fd67c954-592a-48cd-b875-d213fe08799f"
            },
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "markdown",
            "source": [
                "Calculate the average value of prediction."
            ],
            "metadata": {
                "azdata_cell_guid": "1d76155f-b8b6-4957-a08d-96a2f8cdae18"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "pred.select(\"prediction\").agg({\"prediction\": \"avg\"}).show()"
            ],
            "metadata": {
                "azdata_cell_guid": "7d757ce3-0aa6-43b6-a079-a5c6de54cf2e"
            },
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "code",
            "source": [
                "## save and load the model with ML persistence\r\n",
                "# https://spark.apache.org/docs/latest/ml-pipeline.html#ml-persistence-saving-and-loading-pipelines\r\n",
                "\r\n",
                "##NOTE: by default the model is saved to and loaded from hdfs\r\n",
                "model_name = \"battery_life.mml\"\r\n",
                "model_fs = os.path.join(hdfs_path, model_name)\r\n",
                "\r\n",
                "model.write().overwrite().save(model_fs)\r\n",
                "print(\"saved model to {}\".format(model_fs))\r\n",
                "\r\n",
                "# load the model file (from hdfs)\r\n",
                "print(\"load pyspark model from hdfs\")\r\n",
                "model_loaded = PipelineModel.load(model_fs)\r\n",
                "assert str(model_loaded) == str(model)\r\n",
                "\r\n",
                "print(\"loaded model from {}\".format(model_fs))\r\n",
                "print(\"Model is \" , model_loaded)\r\n",
                "print(\"Model stages\", model_loaded.stages)"
            ],
            "metadata": {
                "azdata_cell_guid": "2a3b1e18-ea98-420c-82c1-94ec73fa98c9"
            },
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "code",
            "source": [
                "## export and import model with mleap\r\n",
                "\r\n",
                "import mleap.pyspark\r\n",
                "from mleap.pyspark.spark_support import SimpleSparkSerializer\r\n",
                "\r\n",
                "# serialize the model to a local zip file in JSON format\r\n",
                "#model_name_export = \"adult_census_pipeline.zip\"\r\n",
                "model_name_path = cwd\r\n",
                "model_file = os.path.join(\"/root\", model_name_export)\r\n",
                "\r\n",
                "# remove an old model file, if needed.\r\n",
                "if os.path.isfile(model_file):\r\n",
                "    os.remove(model_file)\r\n",
                "\r\n",
                "model_file_path = \"jar:file:{}\".format(model_file)\r\n",
                "model.serializeToBundle(model_file_path, model.transform(train))\r\n",
                "\r\n",
                "## import mleap model\r\n",
                "model_deserialized = PipelineModel.deserializeFromBundle(model_file_path)\r\n",
                "assert str(model_deserialized) == str(model)\r\n",
                "\r\n",
                "print(\"The deserialized model is \", model_deserialized)\r\n",
                "print(\"The deserialized model stages are\", model_deserialized.stages)\r\n",
                "print(\"The deserialized model is in\", model_file_path)"
            ],
            "metadata": {
                "azdata_cell_guid": "40a035ba-e2cd-41c5-a897-3a4616446b5e"
            },
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "markdown",
            "source": [
                "## **Next Steps: Continue on deploying model to SQL Server big data cluster**\r\n",
                "Now you're ready to open the Jupyter Notebook in this tutorial series - `bdc_tutorial_08.ipynb` - to learn how to export Spark machine learning model and deploy it to SQL Server big data cluster."
            ],
            "metadata": {
                "azdata_cell_guid": "c7122be0-61a8-4b8d-a023-14212908269d"
            }
        }
    ]
}