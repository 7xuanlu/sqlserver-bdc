{
    "metadata": {
        "kernelspec": {
            "name": "pysparkkernel",
            "display_name": "PySpark"
        },
        "language_info": {
            "name": "pyspark",
            "mimetype": "text/x-python",
            "codemirror_mode": {
                "name": "python",
                "version": 2
            },
            "pygments_lexer": "python2"
        }
    },
    "nbformat_minor": 2,
    "nbformat": 4,
    "cells": [
        {
            "cell_type": "markdown",
            "source": [
                "<img src=\"https://github.com/Microsoft/sqlworkshops/blob/master/graphics/solutions-microsoft-logo-small.png?raw=true\" alt=\"Microsoft\">\r\n",
                "<br>\r\n",
                "\r\n",
                "# **SQL Server 2019 big data cluster Tutorial**\r\n",
                "## **07 - Using Spark For Machine Learning**\r\n",
                "\r\n",
                "In this tutorial you will learn how to work with Spark Jobs in a SQL Server big data cluster. \r\n",
                "\r\n",
                "Wide World Importers has refridgerated trucks to deliver temperature-sensitive products. These are high-profit, and high-expense items. In the past, there have been failures in the cooling systems, and the primary culprit has been the deep-cycle batteries used in the system.\r\n",
                "\r\n",
                "WWI began replacing the batteriess every three months as a preventative measure, but this has a high cost. Recently, the taxes on recycling batteries has increased dramatically. The CEO has asked the Data Science team if they can investigate creating a Predictive Maintenance system to more accurately tell the maintenance staff how long a battery will last, rather than relying on a flat 3 month cycle. \r\n",
                "\r\n",
                "The trucks have sensors that transmit data to a file location. The trips are also logged. In this Jupyter Notebook, you'll create, train and store a Machine Learning model using SciKit-Learn, so that it can be deployed to multiple hosts. "
            ],
            "metadata": {
                "azdata_cell_guid": "969bbd54-5f8e-49eb-b466-5e05633fa7be"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "print(\"hello\")"
            ],
            "metadata": {
                "azdata_cell_guid": "f7398c14-8f4b-4cba-bad9-be7f45ed6765"
            },
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": "Starting Spark application\n"
                },
                {
                    "output_type": "display_data",
                    "data": {
                        "text/plain": "<IPython.core.display.HTML object>",
                        "text/html": "<table>\n<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>11</td><td>application_1577707785736_0012</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"https://20.43.162.163:30443/gateway/default/yarn/proxy/application_1577707785736_0012/\">Link</a></td><td><a target=\"_blank\" href=\"https://20.43.162.163:30443/gateway/default/yarn/container/container_1577707785736_0012_01_000001/root\">Link</a></td><td>✔</td></tr></table>"
                    },
                    "metadata": {}
                },
                {
                    "output_type": "display_data",
                    "data": {
                        "text/plain": "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…",
                        "application/vnd.jupyter.widget-view+json": {
                            "version_major": 2,
                            "version_minor": 0,
                            "model_id": "79db336d82454d8298e97597bcc240d4"
                        }
                    },
                    "metadata": {}
                },
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": "SparkSession available as 'spark'.\n"
                },
                {
                    "output_type": "display_data",
                    "data": {
                        "text/plain": "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…",
                        "application/vnd.jupyter.widget-view+json": {
                            "version_major": 2,
                            "version_minor": 0,
                            "model_id": "68a74be9514e45078e9e01537689f355"
                        }
                    },
                    "metadata": {}
                },
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": "hello"
                }
            ],
            "execution_count": 3
        },
        {
            "cell_type": "code",
            "source": [
                "## train a pyspark model and export it as a mleap bundle\r\n",
                "import os\r\n",
                "import numpy as np\r\n",
                "\r\n",
                "# parse command line arguments\r\n",
                "import argparse\r\n",
                "parser = argparse.ArgumentParser(description = 'train pyspark model and export mleap bundle')\r\n",
                "parser.add_argument('hdfs_path', nargs='?', default = \"/spark_ml\", type = str)\r\n",
                "parser.add_argument('model_name_export', nargs='?', default = \"battery_life_pipeline.zip\", type = str)\r\n",
                "args = parser.parse_args()\r\n",
                "\r\n",
                "hdfs_path = args.hdfs_path\r\n",
                "model_name_export = args.model_name_export"
            ],
            "metadata": {
                "azdata_cell_guid": "b6f5169c-8873-4059-90c6-20a4ea473604"
            },
            "outputs": [
                {
                    "output_type": "display_data",
                    "data": {
                        "text/plain": "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…",
                        "application/vnd.jupyter.widget-view+json": {
                            "version_major": 2,
                            "version_minor": 0,
                            "model_id": "61a480554b964faf8d511385b821321a"
                        }
                    },
                    "metadata": {}
                }
            ],
            "execution_count": 8
        },
        {
            "cell_type": "code",
            "source": [
                "# read the data into a spark data frame.\r\n",
                "cwd = os.getcwd()\r\n",
                "filename = \"sensor-data.csv\"\r\n",
                "\r\n",
                "## NOTE: reading text file from local file path seems flaky!\r\n",
                "#import urllib.request\r\n",
                "#url = \"https://amldockerdatasets.azureedge.net/\" + filename\r\n",
                "#local_filename, headers = urllib.request.urlretrieve(url, filename)\r\n",
                "#datafile = \"file://\" + os.path.join(cwd, filename)\r\n",
                "\r\n",
                "data_all = spark.read\\\r\n",
                "    .options(\r\n",
                "        header='true', \r\n",
                "        inferSchema='true', \r\n",
                "        ignoreLeadingWhiteSpace='true', \r\n",
                "        ignoreTrailingWhiteSpace='true')\\\r\n",
                "    .csv(filename) #.load(datafile) for local file\r\n",
                "\r\n",
                "print(\"Number of rows: {},  Number of coulumns : {}\".format(data_all.count(), len(data_all.columns)))"
            ],
            "metadata": {
                "azdata_cell_guid": "fcd76dbf-0418-402e-8c07-9cc6266dc317"
            },
            "outputs": [
                {
                    "output_type": "display_data",
                    "data": {
                        "text/plain": "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…",
                        "application/vnd.jupyter.widget-view+json": {
                            "version_major": 2,
                            "version_minor": 0,
                            "model_id": "ebc681770853410a8b70a81ce9e67574"
                        }
                    },
                    "metadata": {}
                },
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": "Number of rows: 10000,  Number of coulumns : 74"
                }
            ],
            "execution_count": 9
        },
        {
            "cell_type": "code",
            "source": [
                "data_all.printSchema() "
            ],
            "metadata": {
                "azdata_cell_guid": "fb469f1c-a4da-48ae-87f0-f9bf8a2e42bc"
            },
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "markdown",
            "source": [
                "First, download the sensor data from the location where it is transmitted from the trucks, and load it into a Spark DataFrame."
            ],
            "metadata": {
                "azdata_cell_guid": "18f250c3-d1fd-40e1-a652-10f948c8b0ab"
            }
        },
        {
            "cell_type": "markdown",
            "source": [
                "After examining the data, the Data Science team selects certain columns that they believe are highly predictive of the battery life."
            ],
            "metadata": {
                "azdata_cell_guid": "12249759-7afa-402a-badc-9167ad70b5f1"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "# choose feature columns and the label column for training.\r\n",
                "label = data_all.columns[0]\r\n",
                "xvars = data_all.columns[2:7]\r\n",
                "xvars.extend(data_all.columns[9:73])\r\n",
                "\r\n",
                "print(\"label: {}, features: {}\".format(label, xvars))\r\n",
                "\r\n",
                "select_cols = xvars\r\n",
                "select_cols.append(label)\r\n",
                "data = data_all.select(select_cols)"
            ],
            "metadata": {
                "azdata_cell_guid": "4a3781e2-1fb7-46f2-84c1-e23b74b769ae"
            },
            "outputs": [
                {
                    "output_type": "display_data",
                    "data": {
                        "text/plain": "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…",
                        "application/vnd.jupyter.widget-view+json": {
                            "version_major": 2,
                            "version_minor": 0,
                            "model_id": "c1296aaf862d4f61912d766d28495c72"
                        }
                    },
                    "metadata": {}
                },
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": "label: Survival_In_Days, features: ['Region', 'Trip_Length_Mean', 'Trip_Length_Sigma', 'Trips_Per_Day_Mean', 'Trips_Per_Day_Sigma', 'Manufacture_Year', 'Alternator_Efficiency', 'Car_Has_EcoStart', 'Twelve_hourly_temperature_history_for_last_31_days_before_death_last_recording_first', 'Sensor_Reading_1', 'Sensor_Reading_2', 'Sensor_Reading_3', 'Sensor_Reading_4', 'Sensor_Reading_5', 'Sensor_Reading_6', 'Sensor_Reading_7', 'Sensor_Reading_8', 'Sensor_Reading_9', 'Sensor_Reading_10', 'Sensor_Reading_11', 'Sensor_Reading_12', 'Sensor_Reading_13', 'Sensor_Reading_14', 'Sensor_Reading_15', 'Sensor_Reading_16', 'Sensor_Reading_17', 'Sensor_Reading_18', 'Sensor_Reading_19', 'Sensor_Reading_20', 'Sensor_Reading_21', 'Sensor_Reading_22', 'Sensor_Reading_23', 'Sensor_Reading_24', 'Sensor_Reading_25', 'Sensor_Reading_26', 'Sensor_Reading_27', 'Sensor_Reading_28', 'Sensor_Reading_29', 'Sensor_Reading_30', 'Sensor_Reading_31', 'Sensor_Reading_32', 'Sensor_Reading_33', 'Sensor_Reading_34', 'Sensor_Reading_35', 'Sensor_Reading_36', 'Sensor_Reading_37', 'Sensor_Reading_38', 'Sensor_Reading_39', 'Sensor_Reading_40', 'Sensor_Reading_41', 'Sensor_Reading_42', 'Sensor_Reading_43', 'Sensor_Reading_44', 'Sensor_Reading_45', 'Sensor_Reading_46', 'Sensor_Reading_47', 'Sensor_Reading_48', 'Sensor_Reading_49', 'Sensor_Reading_50', 'Sensor_Reading_51', 'Sensor_Reading_52', 'Sensor_Reading_53', 'Sensor_Reading_54', 'Sensor_Reading_55', 'Sensor_Reading_56', 'Sensor_Reading_57', 'Sensor_Reading_58', 'Sensor_Reading_59', 'Sensor_Reading_60']"
                }
            ],
            "execution_count": 10
        },
        {
            "cell_type": "code",
            "source": [
                "## split data into train and test.\r\n",
                "\r\n",
                "train, test = data.randomSplit([0.75, 0.25], seed=123)\r\n",
                "\r\n",
                "print(\"train ({}, {})\".format(train.count(), len(train.columns)))\r\n",
                "print(\"test ({}, {})\".format(test.count(), len(test.columns)))\r\n",
                "\r\n",
                "train_data_path = os.path.join(hdfs_path, \"sensor-data-train\")\r\n",
                "test_data_path = os.path.join(hdfs_path, \"sensor-data-test\")\r\n",
                "\r\n",
                "# write the train and test data sets to intermediate storage and then read\r\n",
                "train.write.mode('overwrite').orc(train_data_path)\r\n",
                "test.write.mode('overwrite').orc(test_data_path)\r\n",
                "\r\n",
                "print(\"train and test datasets saved to {} and {}\".format(train_data_path, test_data_path))\r\n",
                "\r\n",
                "train_read = spark.read.orc(train_data_path)\r\n",
                "test_read = spark.read.orc(test_data_path)\r\n",
                "\r\n",
                "assert train_read.schema == train.schema and train_read.count() == train.count() \r\n",
                "assert test_read.schema == test.schema and test_read.count() == test.count()\r\n",
                ""
            ],
            "metadata": {
                "azdata_cell_guid": "a0599280-eb9f-4c84-9080-c3bf4197b6fc"
            },
            "outputs": [
                {
                    "output_type": "display_data",
                    "data": {
                        "text/plain": "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…",
                        "application/vnd.jupyter.widget-view+json": {
                            "version_major": 2,
                            "version_minor": 0,
                            "model_id": "d840467ec5794c63aa01c3b4f90a7bc9"
                        }
                    },
                    "metadata": {}
                },
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": "train (7479, 70)\ntest (2521, 70)\ntrain and test datasets saved to /spark_ml/sensor-data-train and /spark_ml/sensor-data-test"
                }
            ],
            "execution_count": 11
        },
        {
            "cell_type": "code",
            "source": [
                "from pyspark.ml import Pipeline, PipelineModel\r\n",
                "from pyspark.ml.feature import StringIndexer, VectorAssembler\r\n",
                "from pyspark.ml.regression import GBTRegressor\r\n",
                "from pyspark.ml.evaluation import RegressionEvaluator\r\n",
                "\r\n",
                "# Convert columns with string type to indexed string type\r\n",
                "str_indexer_1 = StringIndexer(inputCol=\"Region\", outputCol=\"Reg\")\r\n",
                "str_indexer_2 = StringIndexer(inputCol=\"Manufacture_Year\", outputCol=\"Manu_Year\")\r\n",
                "\r\n",
                "# Add newly created columns\r\n",
                "xvars.append(\"Reg\")\r\n",
                "xvars.append(\"Manu_Year\")\r\n",
                "\r\n",
                "# Remove duplicate columns\r\n",
                "xvars.remove(\"Region\")\r\n",
                "xvars.remove(\"Manufacture_Year\")\r\n",
                "\r\n",
                "# Assemble the encoded feature columns in to a column named \"features\"\r\n",
                "assembler = VectorAssembler(inputCols=xvars, outputCol=\"features\", handleInvalid=\"skip\")\r\n",
                "\r\n",
                "# Create a Gradient Boosted Tree Reression model, which by default uses \"features\" and \"label\" columns for training\r\n",
                "gbt = GBTRegressor(labelCol=label)\r\n",
                "\r\n",
                "# Put together the pipeline\r\n",
                "stages = []\r\n",
                "stages.append(str_indexer_1)\r\n",
                "stages.append(str_indexer_2)\r\n",
                "stages.append(assembler)\r\n",
                "stages.append(gbt)\r\n",
                "\r\n",
                "pipe = Pipeline(stages=stages)\r\n",
                "print(\"Pipeline created\")\r\n",
                "\r\n",
                "# Train the model.\r\n",
                "model = pipe.fit(train_read)\r\n",
                "print(\"Model Trained\")\r\n",
                "print(\"Model is \", model)\r\n",
                "print(\"Model Stages\", model.stages)"
            ],
            "metadata": {
                "azdata_cell_guid": "d6da7ab5-ea75-4e16-9655-a3c86f63da9a"
            },
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "code",
            "source": [
                "from pyspark.ml import PipelineModel\r\n",
                "model = PipelineModel.load(\"/spark_ml/battery_life.mml\")"
            ],
            "metadata": {
                "azdata_cell_guid": "7fcc5283-3e1e-4151-912b-c2d3627bcf06"
            },
            "outputs": [
                {
                    "output_type": "display_data",
                    "data": {
                        "text/plain": "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…",
                        "application/vnd.jupyter.widget-view+json": {
                            "version_major": 2,
                            "version_minor": 0,
                            "model_id": "695776e518e14e949ea956e12650b45a"
                        }
                    },
                    "metadata": {}
                }
            ],
            "execution_count": 6
        },
        {
            "cell_type": "markdown",
            "source": [
                "The lead Data Scientist believes that a standard Regression algorithm would do the best predictions."
            ],
            "metadata": {
                "azdata_cell_guid": "88ff4ca6-d8ec-49ed-8e4e-915f1664527e"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "# Try making a single prediction and observe the result \r\n",
                "pred = model.transform(test_read)"
            ],
            "metadata": {
                "azdata_cell_guid": "11818008-b450-4799-a779-a1b6de81093b"
            },
            "outputs": [
                {
                    "output_type": "display_data",
                    "data": {
                        "text/plain": "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…",
                        "application/vnd.jupyter.widget-view+json": {
                            "version_major": 2,
                            "version_minor": 0,
                            "model_id": "65594d8ca9c9435b8b869346e91adf2b"
                        }
                    },
                    "metadata": {}
                },
                {
                    "output_type": "stream",
                    "name": "stderr",
                    "text": "name 'test_read' is not defined\nTraceback (most recent call last):\nNameError: name 'test_read' is not defined\n\n"
                }
            ],
            "execution_count": 7
        },
        {
            "cell_type": "markdown",
            "source": [
                "After the model is trained, perform testing from labeled data."
            ],
            "metadata": {
                "azdata_cell_guid": "0e6b111e-8e2f-41bc-a4f2-ed0620b147d6"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "from pyspark.ml.evaluation import RegressionEvaluator\r\n",
                "evaluator = RegressionEvaluator(predictionCol=\"prediction\", labelCol=label)\r\n",
                "print(\"Root mean squared error(RMSE) on test data = %g\" % evaluator.evaluate(pred))"
            ],
            "metadata": {
                "azdata_cell_guid": "fd67c954-592a-48cd-b875-d213fe08799f"
            },
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "code",
            "source": [
                "pred.select(\"prediction\").agg({\"prediction\": \"avg\"}).show()"
            ],
            "metadata": {
                "azdata_cell_guid": "7d757ce3-0aa6-43b6-a079-a5c6de54cf2e"
            },
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "code",
            "source": [
                "## save and load the model with ML persistence\r\n",
                "# https://spark.apache.org/docs/latest/ml-pipeline.html#ml-persistence-saving-and-loading-pipelines\r\n",
                "\r\n",
                "##NOTE: by default the model is saved to and loaded from hdfs\r\n",
                "model_name = \"battery_life.mml\"\r\n",
                "model_fs = os.path.join(hdfs_path, model_name)\r\n",
                "\r\n",
                "model.write().overwrite().save(model_fs)\r\n",
                "print(\"saved model to {}\".format(model_fs))\r\n",
                "\r\n",
                "# load the model file (from hdfs)\r\n",
                "print(\"load pyspark model from hdfs\")\r\n",
                "model_loaded = PipelineModel.load(model_fs)\r\n",
                "assert str(model_loaded) == str(model)\r\n",
                "\r\n",
                "print(\"loaded model from {}\".format(model_fs))\r\n",
                "print(\"Model is \" , model_loaded)\r\n",
                "print(\"Model stages\", model_loaded.stages)"
            ],
            "metadata": {
                "azdata_cell_guid": "2a3b1e18-ea98-420c-82c1-94ec73fa98c9"
            },
            "outputs": [
                {
                    "output_type": "display_data",
                    "data": {
                        "text/plain": "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…",
                        "application/vnd.jupyter.widget-view+json": {
                            "version_major": 2,
                            "version_minor": 0,
                            "model_id": "e137b5bceb3745df8fb54de004cf7172"
                        }
                    },
                    "metadata": {}
                },
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": "load pyspark model from hdfs\nloaded model from /spark_ml/battery_life.mml\nModel is  PipelineModel_0dafc665b7d5\nModel stages [StringIndexer_533e05324d90, StringIndexer_a6c280ca4904, VectorAssembler_0798816bf05a, GBTRegressionModel (uid=GBTRegressor_7b7961fd0d0f) with 20 trees]"
                }
            ],
            "execution_count": 9
        },
        {
            "cell_type": "code",
            "source": [
                "## export and import model with mleap\r\n",
                "\r\n",
                "import mleap.pyspark\r\n",
                "from mleap.pyspark.spark_support import SimpleSparkSerializer\r\n",
                "\r\n",
                "# serialize the model to a local zip file in JSON format\r\n",
                "#model_name_export = \"adult_census_pipeline.zip\"\r\n",
                "model_name_path = cwd\r\n",
                "model_file = os.path.join(\"/root\", model_name_export)\r\n",
                "\r\n",
                "# remove an old model file, if needed.\r\n",
                "if os.path.isfile(model_file):\r\n",
                "    os.remove(model_file)\r\n",
                "\r\n",
                "model_file_path = \"jar:file:{}\".format(model_file)\r\n",
                "model.serializeToBundle(model_file_path, model.transform(train))\r\n",
                "\r\n",
                "## import mleap model\r\n",
                "model_deserialized = PipelineModel.deserializeFromBundle(model_file_path)\r\n",
                "assert str(model_deserialized) == str(model)\r\n",
                "\r\n",
                "print(\"The deserialized model is \", model_deserialized)\r\n",
                "print(\"The deserialized model stages are\", model_deserialized.stages)\r\n",
                "print(\"The deserialized model is in\", model_file_path)"
            ],
            "metadata": {
                "azdata_cell_guid": "40a035ba-e2cd-41c5-a897-3a4616446b5e"
            },
            "outputs": [
                {
                    "output_type": "display_data",
                    "data": {
                        "text/plain": "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…",
                        "application/vnd.jupyter.widget-view+json": {
                            "version_major": 2,
                            "version_minor": 0,
                            "model_id": "0a093cb977bc4174a3883d0b17c7e22b"
                        }
                    },
                    "metadata": {}
                },
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": "The deserialized model is  PipelineModel_0dafc665b7d5\nThe deserialized model stages are [StringIndexer_533e05324d90, StringIndexer_a6c280ca4904, VectorAssembler_0798816bf05a, GBTRegressionModel (uid=GBTRegressor_7b7961fd0d0f) with 20 trees]\nThe deserialized model is in jar:file:/root/battery_life_pipeline.zip"
                }
            ],
            "execution_count": 42
        },
        {
            "cell_type": "code",
            "source": [
                "model_file"
            ],
            "metadata": {
                "azdata_cell_guid": "4aaa6c10-f4c2-4fd3-8e24-48f8c437f461"
            },
            "outputs": [
                {
                    "output_type": "display_data",
                    "data": {
                        "text/plain": "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…",
                        "application/vnd.jupyter.widget-view+json": {
                            "version_major": 2,
                            "version_minor": 0,
                            "model_id": "3f867382ec0d475b8453ecde980a8527"
                        }
                    },
                    "metadata": {}
                },
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": "'/root/battery_life_pipeline.zip'"
                }
            ],
            "execution_count": 44
        },
        {
            "cell_type": "markdown",
            "source": [
                "## **Next Steps: Continue on to other workloads in SQL Server 2019**\r\n",
                "Now you're ready to work with SQL Server 2019's other features - [you can learn more about those here](https://docs.microsoft.com/en-us/sql/big-data-cluster/big-data-cluster-overview?view=sqlallproducts-allversions)."
            ],
            "metadata": {
                "azdata_cell_guid": "c7122be0-61a8-4b8d-a023-14212908269d"
            }
        }
    ]
}