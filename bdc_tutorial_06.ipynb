{
    "metadata": {
        "kernelspec": {
            "name": "pysparkkernel",
            "display_name": "PySpark"
        },
        "language_info": {
            "name": "pyspark",
            "mimetype": "text/x-python",
            "codemirror_mode": {
                "name": "python",
                "version": 2
            },
            "pygments_lexer": "python2"
        }
    },
    "nbformat_minor": 2,
    "nbformat": 4,
    "cells": [
        {
            "cell_type": "markdown",
            "source": [
                "<img src=\"https://github.com/Microsoft/sqlworkshops/blob/master/graphics/solutions-microsoft-logo-small.png?raw=true\" alt=\"Microsoft\">\r\n",
                "<br>\r\n",
                "\r\n",
                "# **SQL Server 2019 big data cluster Tutorial**\r\n",
                "## **06 - Using Spark for ETL**\r\n",
                "\r\n",
                "In this tutorial you will learn how to work with Spark Jobs in a SQL Server big data cluster. \r\n",
                "\r\n",
                "Many times Spark is used to do transformations on data at large scale. In this Jupyter Notebook, you'll read a large text file into a Spark DataFrame, and then save out the top 10 examples as a table using SparkSQL.\r\n",
                "\r\n",
                "> Switch your kernel to PySpark and run print(\"hello\") or whatever you like to activate Spark context.  \r\n",
                ">\r\n",
                "> If it output error like this\r\n",
                "> ```\r\n",
                "> The code failed because of a fatal error:\r\n",
                ">   Error sending http request and maximum retry encountered..\r\n",
                "> ```\r\n",
                "> Please switch to another kernel and switch back, and run again."
            ],
            "metadata": {
                "azdata_cell_guid": "af570b3d-c696-4d29-9290-c5b5d042660f"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "# Test if SparkSession is available as 'spark'\r\n",
                "print('hello')"
            ],
            "metadata": {
                "azdata_cell_guid": "a5ac36e5-7869-4799-9e32-ff79478df18c"
            },
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": "Starting Spark application\n"
                },
                {
                    "output_type": "display_data",
                    "data": {
                        "text/plain": "<IPython.core.display.HTML object>",
                        "text/html": "<table>\n<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>4</td><td>application_1577707785736_0005</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"https://20.43.162.163:30443/gateway/default/yarn/proxy/application_1577707785736_0005/\">Link</a></td><td><a target=\"_blank\" href=\"https://20.43.162.163:30443/gateway/default/yarn/container/container_1577707785736_0005_01_000001/root\">Link</a></td><td>✔</td></tr></table>"
                    },
                    "metadata": {}
                },
                {
                    "output_type": "display_data",
                    "data": {
                        "text/plain": "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…",
                        "application/vnd.jupyter.widget-view+json": {
                            "version_major": 2,
                            "version_minor": 0,
                            "model_id": "cd798eba75df401894c4437e8d3410f4"
                        }
                    },
                    "metadata": {}
                },
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": "SparkSession available as 'spark'.\n"
                },
                {
                    "output_type": "display_data",
                    "data": {
                        "text/plain": "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…",
                        "application/vnd.jupyter.widget-view+json": {
                            "version_major": 2,
                            "version_minor": 0,
                            "model_id": "e0c865745c8f49aaaec0585a84a93dac"
                        }
                    },
                    "metadata": {}
                },
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": "hello"
                }
            ],
            "execution_count": 3
        },
        {
            "cell_type": "code",
            "source": [
                "# Read the product reviews CSV files into a spark data frame, print schema & top rows\r\n",
                "results = spark.read.option(\"inferSchema\", \"true\").csv('/product_review_data').toDF(\"Item_ID\", \"Review\")\r\n",
                "results.show()"
            ],
            "metadata": {
                "azdata_cell_guid": "7fe4f671-195f-4ce2-9d08-45c9ff175cb0"
            },
            "outputs": [
                {
                    "output_type": "display_data",
                    "data": {
                        "text/plain": "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…",
                        "application/vnd.jupyter.widget-view+json": {
                            "version_major": 2,
                            "version_minor": 0,
                            "model_id": "0f6e192b3d0c49348e50bd8ef4242e56"
                        }
                    },
                    "metadata": {}
                },
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": "+-------+--------------------+\n|Item_ID|              Review|\n+-------+--------------------+\n|  72621|Works fine. Easy ...|\n|  89334|great product to ...|\n|  89335|Next time will go...|\n|  84259|Great Gift Great ...|\n|  84398|After trip to Par...|\n|  66434|Simply the best t...|\n|  66501|This is the exact...|\n|  66587|Not super magnet;...|\n|  66680|Installed as bath...|\n|  66694|Our home was buil...|\n|  84489|Hi ;We are runnin...|\n|  79052|Terra cotta is th...|\n|  73034|One of my fingern...|\n|  73298|We installed thes...|\n|  66810|needed silicone c...|\n|  66912|Great Gift Great ...|\n|  67028|Laguiole knives a...|\n|  89770|Good sound timers...|\n|  84679|AWESOME FEEDBACK ...|\n|  84953|love the retro gl...|\n+-------+--------------------+\nonly showing top 20 rows"
                }
            ],
            "execution_count": 5
        },
        {
            "cell_type": "code",
            "source": [
                "# Save results as parquet file and create hive table\r\n",
                "results.write.format(\"parquet\").mode(\"overwrite\").saveAsTable(\"Top_Product_Reviews\")"
            ],
            "metadata": {
                "azdata_cell_guid": "59bdf34a-a397-4ed0-80ad-523768141073"
            },
            "outputs": [
                {
                    "output_type": "display_data",
                    "data": {
                        "text/plain": "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…",
                        "application/vnd.jupyter.widget-view+json": {
                            "version_major": 2,
                            "version_minor": 0,
                            "model_id": "1620c39607354dc384d2f73f4aae1034"
                        }
                    },
                    "metadata": {}
                }
            ],
            "execution_count": 6
        },
        {
            "cell_type": "code",
            "source": [
                "# Execute Spark SQL commands\r\n",
                "sqlDF = spark.sql(\"SELECT * FROM Top_Product_Reviews LIMIT 10\")\r\n",
                "sqlDF.show()"
            ],
            "metadata": {
                "azdata_cell_guid": "77ec4ac7-4a92-4bad-b170-a3e2568ae6f3"
            },
            "outputs": [
                {
                    "output_type": "display_data",
                    "data": {
                        "text/plain": "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…",
                        "application/vnd.jupyter.widget-view+json": {
                            "version_major": 2,
                            "version_minor": 0,
                            "model_id": "938e5074ecfd4220b9d7672ea02f1309"
                        }
                    },
                    "metadata": {}
                },
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": "+-------+--------------------+\n|Item_ID|              Review|\n+-------+--------------------+\n|  72621|Works fine. Easy ...|\n|  89334|great product to ...|\n|  89335|Next time will go...|\n|  84259|Great Gift Great ...|\n|  84398|After trip to Par...|\n|  66434|Simply the best t...|\n|  66501|This is the exact...|\n|  66587|Not super magnet;...|\n|  66680|Installed as bath...|\n|  66694|Our home was buil...|\n+-------+--------------------+"
                }
            ],
            "execution_count": 7
        },
        {
            "cell_type": "markdown",
            "source": [
                "## **Next Steps: Continue on to Working with Spark and Machine Learning**\r\n",
                "\r\n",
                "Now you're ready to open the final Python Notebook in this tutorial series - `bdc_tutorial_07.ipynb` - to learn how to create and work with Spark and Machine Learning."
            ],
            "metadata": {
                "azdata_cell_guid": "483977c9-a81c-4ec9-9290-40e4ba586e9e"
            }
        }
    ]
}